{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "generous-kennedy",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Trim raw reads using trimmomatic \n",
    "-------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-comparative",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "cd /path/to/raw/reads\n",
    "\n",
    "for R1 in *R1*\n",
    "do\n",
    "   R2=${R1//R1.fastq.gz/R2.fastq.gz}\n",
    "   R1paired=${R1//.fastq.gz/_paired.fastq.gz}\n",
    "   R1unpaired=${R1//.fastq.gz/_unpaired.fastq.gz}\n",
    "   R2paired=${R2//.fastq.gz/_paired.fastq.gz}\n",
    "   R2unpaired=${R2//.fastq.gz/_unpaired.fastq.gz} \\\n",
    "   java -jar trimmomatic-0.39.jar PE $R1 $R2 $R1paired $R1unpaired $R2paired $R2unpaired \\\n",
    "   ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:5 TRAILING:5 SLIDINGWINDOW:4:15 MINLEN:100\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-surveillance",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## RUN FASTQC #TODOOOOOO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-instrument",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Assembly with SPADES \n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-modern",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "for file1 in /path/to/trimmed/data/directory/*R1*fastq\n",
    "do \n",
    "file2=${file1/R1/R2}\n",
    "out=${file1%%_R1.fastq}_output\n",
    "spades.py -k 21,33,55,77 -c --only-assembler -1 $file1 -2 $file2 -o $out\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-angel",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Ref Alignment \n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-passage",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "##John  Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-subject",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## SV Calling \n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-continuity",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "**python script for making pindel config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-personality",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "for file in os.listdir(\"/home/sharrison/data/bams/\"):\n",
    "    bam = \".\"\n",
    "    if file.endswith(\".stats\"):\n",
    "        mySample = file.split(\".\")[0]\n",
    "        theBam = file.split(\".\")\n",
    "        theBam = theBam[:-1]\n",
    "        print(theBam)\n",
    "        bam = bam.join(theBam)\n",
    "        myBam = \"/home/sharrison/data/bams/\" + file \n",
    "        file1 = open(myBam, \"r\")\n",
    "        print(myBam)\n",
    "        for line in file1.readlines():\n",
    "            if line.startswith(\"SN\"):\n",
    "                if line.split(\"\\t\")[1] == \"insert size average:\":\n",
    "                    insertSize = line.rstrip().split(\"\\t\")[2]\n",
    "        myText = \"/home/sharrison/data/bams/\" + bam + \"\\t\" + insertSize + \"\\t\" + mySample\n",
    "        newName = mySample + \"_pindel.cfg\"\n",
    "        pFile = open(newName, \"w\")\n",
    "        pFile.write(myText)\n",
    "        pFile.close()\n",
    "        file1.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-alcohol",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "**Python script for making script to run lumpy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-rescue",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "myFile = \"#!/bin/bash\" + \"\\n\"\n",
    "for file in os.listdir(\"/home/sharrison/data/bams/\"):\n",
    "    bam = \".\"\n",
    "    if file.endswith(\".stats\"):\n",
    "        mySample = file.split(\".\")[0]\n",
    "        theBam = file.split(\".\")\n",
    "        theBam = theBam[:-1]\n",
    "        bam = bam.join(theBam)\n",
    "        myBam = \"/home/sharrison/data/bams/\" + file \n",
    "        file1 = open(myBam, \"r\")\n",
    "        for line in file1.readlines():\n",
    "            if line.startswith(\"SN\"):\n",
    "                if line.split(\"\\t\")[1] == \"insert size average:\":\n",
    "                    insertSize = line.rstrip().split(\"\\t\")[2]\n",
    "                if line.split(\"\\t\")[1] == \"insert size standard deviation:\":\n",
    "                    sd = line.rstrip().split(\"\\t\")[2]\n",
    "                if line.split(\"\\t\")[1] == \"average length:\":\n",
    "                    readLen = line.rstrip().split(\"\\t\")[2]\n",
    "        myText = \"lumpy -mw 4 -tt 0 -pe id:\" + mySample + \",bam_file:\" + mySample + \".discordants.bam,\" + \"histo_file:\" + mySample + \".lib1.histo,mean:\" + insertSize + \",stdev:\" + sd + \",read_length:\" + readLen + \",min_non_overlap:\" + readLen + \",discordant_z:5,back_distance:10,weight:1,min_mapping_threshold:20\" + \" -sr id:\" + mySample + \",bam_file:\" + mySample + \".splitters.bam\" + \",back_distance:10,weight:1,min_mapping_threshold:20\" + \" > \" + mySample + \"_lumpy.vcf\" + \"\\n\"\n",
    "        myFile += myText\n",
    "\n",
    "\n",
    "pFile = open(\"runLumpy.sh\", \"w\")\n",
    "pFile.write(myFile)\n",
    "pFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-feedback",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "**Running all steps**\n",
    "**TODO**\n",
    "- set up t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-dependence",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "\n",
    "##change data to whatever data folder called\n",
    "\n",
    "cd ~/data\n",
    "\n",
    "mkdir pindel\n",
    "mkdir delly\n",
    "mkdir breakdancer\n",
    "mkdir manta\n",
    "mkdir gridss\n",
    "mkdir lumpy \n",
    "\n",
    "##first need to run stats on each sample to properly make config files \n",
    "cd ~/data/bams\n",
    "for num in {1..22}\n",
    "do\n",
    "samtools stats bm{num}.bam > bm{num}.stats\n",
    "done\n",
    "\n",
    "\n",
    "##pindel run pindel.sh script after have cfg files\n",
    "cd ~/data/pindel\n",
    "python3 makePindelCfg.py \n",
    "chmod u+x pindel.sh\n",
    "nohup ./pindel.sh &\n",
    "\n",
    "###breakdancer\n",
    "cd ~/data/breakdancer\n",
    "\n",
    "for num in {1..22}\n",
    "do\n",
    "perl /usr/local/breakdancer/perl/bam2cfg.pl -g -h ~/data/bams/bm${num}.sorted.bam > bm${num}.cfg\n",
    "breakdancer-max -r 4 bm${num}.cfg > bm${num}_breakdancer.txt\n",
    "done\n",
    "\n",
    "###delly \n",
    "cd ~/data/delly\n",
    "\n",
    "for num in {1..22}\n",
    "do\n",
    "delly call -g ~/data/ref/ref246.fa -o bm${num}.bcf ~/data/bams/bm${num}.sorted.bam \n",
    "done\n",
    "\n",
    "delly merge -o all_sites.bcf *.bcf\n",
    "\n",
    "for num in {1..22}\n",
    "do\n",
    "delly call -g ~/data/ref/ref246.fa -v all_sites.bcf -o bm${num}_delly.bcf ~/data/bams/bm${num}.sorted.bam \n",
    "done\n",
    "\n",
    "###manta\n",
    "cd ~/data/manta\n",
    "\n",
    "for num in {11..22}\n",
    "do\n",
    "mkdir bm${num}\n",
    "configManta.py --bam ~/data/bams/bm${num}.sorted.bam --referenceFasta ~/data/ref/ref246.fa --runDir bm${num}\n",
    "~/data/manta/bm${num}/runWorkflow.py\n",
    "done\n",
    "\n",
    "\n",
    "###gridss\n",
    "cd ~/data/gridss\n",
    "for num in {1..22}\n",
    "do\n",
    "gridss.sh --jar /usr/local/gridss/scripts/gridss.jar --reference ~/data/ref/ref246.fa --output bm${num}_gridss.vcf.gz --labels bm${num}  --assembly bm${num}_g.bam ~/data/bams/bm${num}.sorted.bam \n",
    "done\n",
    "\n",
    "\n",
    "###lumpy\n",
    "cd ~/data/lumpy\n",
    "\n",
    "for num in {1..22}\n",
    "do\n",
    "samtools view -b -F 1294 ~/data/bams/bm${num}.sorted.bam > bm${num}.discordants.bam\n",
    "samtools view -h ~/data/bams/bm${num}.sorted.bam \\\n",
    "    | /usr/local/lumpy-sv/scripts/extractSplitReads_BwaMem -i stdin \\\n",
    "    | samtools view -Sb - \\\n",
    "    > bm${num}.splitters.bam\n",
    "done\n",
    "\n",
    "# will probably have to change for 1,11,12 sample because not 100 for sequence reads\n",
    "\n",
    "listS=(2 3 4 5 6 7 8 9 10 13 14 15 16 17 18 19 20 21 22)\n",
    "for num in ${listS[@]}\n",
    "do\n",
    "samtools view ~/data/bams/bm${num}.sorted.bam \\\n",
    "    | tail -n+100000 \\\n",
    "    | /usr/local/lumpy-sv/scripts/pairend_distro.py \\\n",
    "    -r 250 \\\n",
    "    -X 4 \\\n",
    "    -N 10000 \\\n",
    "    -o bm${num}.lib1.histo\n",
    "done\n",
    "\n",
    "listS=(1 11 12)\n",
    "for num in ${listS[@]}\n",
    "do\n",
    "samtools view ~/data/bams/bm${num}.sorted.bam \\\n",
    "    | tail -n+100000 \\\n",
    "    | /usr/local/lumpy-sv/scripts/pairend_distro.py \\\n",
    "    -r 100 \\\n",
    "    -X 4 \\\n",
    "    -N 10000 \\\n",
    "    -o bm${num}.lib1.histo\n",
    "done\n",
    "\n",
    "###run the lumpy script with commands for each sample\n",
    "python3 lumpyRun.sh\n",
    "chmod u+x runLumpy.sh\n",
    "nohup ./runLumpy.sh &\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-opening",
   "metadata": {
    "kernel": "python3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "def read_vcf(path):\n",
    "    with open(path, 'r') as f:\n",
    "        lines = [l for l in f if not l.startswith('##')]\n",
    "    return pd.read_csv(\n",
    "        io.StringIO(''.join(lines)),\n",
    "        dtype={'#CHROM': str, 'POS': int, 'ID': str, 'REF': str, 'ALT': str,\n",
    "               'QUAL': str, 'FILTER': str, 'INFO': str},\n",
    "        sep='\\t'\n",
    "    ).rename(columns={'#CHROM': 'CHROM'})\n",
    "\n",
    "def reformat_breakdancer(file):\n",
    "    unformat_break = pd.read_csv(file, sep='\\t', skiprows=[0,1,2,3])\n",
    "    info_list = []\n",
    "    for index,row in unformat_break.iterrows():\n",
    "        info_line = \"SVLEN=\" + str(row[\"Size\"]) + \":END=\" + str(row[\"Pos2\"]) + \":num_reads=\" + str(row[\"num_Reads\"])\n",
    "        info_list.append(info_line)\n",
    "    new_breakdancer = pd.DataFrame(columns=['#CHROM', 'POS', 'ID', 'REF', 'ALT','QUAL', 'FILTER', 'INFO'])\n",
    "    chrom_list = unformat_break['#Chr1'].to_list()\n",
    "    pos_list = unformat_break['Pos1'].to_list()\n",
    "    alt_list = unformat_break['Type'].to_list()\n",
    "    qual_list = unformat_break['Score'].to_list()\n",
    "    new_breakdancer['#CHROM'] = chrom_list\n",
    "    new_breakdancer['POS'] = pos_list\n",
    "    new_breakdancer['ID'] = \"breakdancer\"\n",
    "    new_breakdancer['REF'] = \"N\"\n",
    "    new_breakdancer['ALT'] = alt_list \n",
    "    new_breakdancer['QUAL'] = qual_list \n",
    "    new_breakdancer['FILTER'] = \".\"\n",
    "    new_breakdancer['INFO'] = info_list\n",
    "    return new_breakdancer \n",
    "\n",
    "\n",
    "def reformat_ss(manta,caller):  \n",
    "    end_list = [] \n",
    "    if caller == 'breakdancer':\n",
    "        for index,row in manta.iterrows():\n",
    "            if row['ALT'] == 'CTX':\n",
    "                manta.drop(index, inplace=True)\n",
    "            else:\n",
    "                end = row['INFO'].split('END=')[1].split(\":\")[0]\n",
    "                end_list.append(end)\n",
    "        chroms = manta['CHROM'].to_list()\n",
    "        start_pos = manta['POS'].to_list()\n",
    "        svtype_list = manta['ALT'].to_list()\n",
    "        qual = manta['QUAL'].to_list()\n",
    "        new_fr = pd.DataFrame(columns=['CHROM', 'START_POS', 'STOP_POS', 'SV_TYPE', 'QUAL', 'CALLER'])\n",
    "        new_fr['CHROM'] = chroms\n",
    "        new_fr['START_POS'] = start_pos\n",
    "        new_fr['STOP_POS'] = end_list\n",
    "        new_fr['SV_TYPE'] = svtype_list\n",
    "        new_fr['QUAL'] = qual\n",
    "        new_fr['CALLER'] = caller\n",
    "        \n",
    "    elif caller == 'pindel':\n",
    "        for index,row in manta.iterrows():\n",
    "            SV_type = row['INFO'].split('SVTYPE=')[1].split(';')[0]\n",
    "            end = row['INFO'].split('END=')[1].split(\";\")[0]\n",
    "            end_list.append(end)\n",
    "            new = SV_type \n",
    "            manta.at[index,'ALT'] = new\n",
    "        chroms = manta['CHROM'].to_list()\n",
    "        start_pos = manta['POS'].to_list()\n",
    "        svtype_list = manta['ALT'].to_list()\n",
    "        new_fr = pd.DataFrame(columns=['CHROM', 'START_POS', 'STOP_POS', 'SV_TYPE', 'QUAL', 'CALLER'])\n",
    "        new_fr['CHROM'] = chroms\n",
    "        new_fr['START_POS'] = start_pos\n",
    "        new_fr['STOP_POS'] = end_list\n",
    "        new_fr['SV_TYPE'] = svtype_list\n",
    "        new_fr['QUAL'] = \".\"\n",
    "        new_fr['CALLER'] = caller\n",
    "        \n",
    "    elif caller == 'delly':\n",
    "        ind_l = []\n",
    "        for index,row in manta.iterrows():\n",
    "            gt = str(manta.iloc[index,9])\n",
    "            gt_l = gt.split(\":\")[0]\n",
    "            if gt_l == '0/0':\n",
    "                ind_l.append(index)\n",
    "        for index,row in manta.iterrows():\n",
    "            SV_type = row['INFO'].split('SVTYPE=')[1].split(';')[0]\n",
    "            if SV_type == \"BND\":\n",
    "                end_list.append(\"BND\")\n",
    "            else:\n",
    "                end = row['INFO'].split('END=')[1].split(\";\")[0]\n",
    "                end_list.append(end)\n",
    "            new = SV_type\n",
    "            manta.at[index,'ALT'] = new\n",
    "        chroms = manta['CHROM'].to_list()\n",
    "        start_pos = manta['POS'].to_list()\n",
    "        svtype_list = manta['ALT'].to_list()\n",
    "        qual = manta['QUAL'].to_list()\n",
    "        new_fr = pd.DataFrame(columns=['CHROM', 'START_POS', 'STOP_POS', 'SV_TYPE', 'QUAL', 'CALLER'])\n",
    "        new_fr['CHROM'] = chroms\n",
    "        new_fr['START_POS'] = start_pos\n",
    "        new_fr['STOP_POS'] = end_list\n",
    "        new_fr['SV_TYPE'] = svtype_list\n",
    "        new_fr['QUAL'] = qual\n",
    "        new_fr['CALLER'] = caller\n",
    "        for num in ind_l:\n",
    "            new_fr.drop(num, inplace=True)\n",
    "    else:\n",
    "\n",
    "        for index,row in manta.iterrows():\n",
    "            SV_type = row['INFO'].split('SVTYPE=')[1].split(';')[0]\n",
    "            if SV_type == \"BND\":\n",
    "                end_list.append(\"BND\")\n",
    "            else:\n",
    "                end = row['INFO'].split('END=')[1].split(\";\")[0]\n",
    "                end_list.append(end)\n",
    "            new = SV_type\n",
    "            manta.at[index,'ALT'] = new\n",
    "        chroms = manta['CHROM'].to_list()\n",
    "        start_pos = manta['POS'].to_list()\n",
    "        svtype_list = manta['ALT'].to_list()\n",
    "        qual = manta['QUAL'].to_list()\n",
    "        new_fr = pd.DataFrame(columns=['CHROM', 'START_POS', 'STOP_POS', 'SV_TYPE', 'QUAL', 'CALLER'])\n",
    "        new_fr['CHROM'] = chroms\n",
    "        new_fr['START_POS'] = start_pos\n",
    "        new_fr['STOP_POS'] = end_list\n",
    "        new_fr['SV_TYPE'] = svtype_list\n",
    "        new_fr['QUAL'] = qual\n",
    "        new_fr['CALLER'] = caller\n",
    "               \n",
    "    return new_fr\n",
    "\n",
    "def combine_SV(breakdancer_file,pindel_file,delly_file,manta_file,lumpy_file,gridss_file):\n",
    "    breakdancer = pd.read_csv(breakdancer_file,sep='\\t')\n",
    "    pindel = pd.read_csv(pindel_file,sep='\\t')\n",
    "    lumpy = pd.read_csv(lumpy_file,sep='\\t')\n",
    "    delly = pd.read_csv(delly_file,sep='\\t')\n",
    "    manta = pd.read_csv(manta_file,sep='\\t')\n",
    "    gridss = pd.read_csv(gridss_file,sep='\\t')\n",
    "    all_types = pd.concat([breakdancer,pindel,lumpy,delly,manta,gridss], axis=0)\n",
    "    return all_types\n",
    "\n",
    "def make_merged(file, max_size, min_size, percent_threshold, caller_threshold):\n",
    "    sample_SV = pd.read_csv(file,sep='\\t')\n",
    "    for index, row in sample_SV.iterrows():\n",
    "        if row['STOP_POS'] == \"BND\":\n",
    "            sample_SV.drop(index, inplace=True)\n",
    "    for index, row in sample_SV.iterrows(): \n",
    "        sv_len = int(row['STOP_POS']) - int(row['START_POS'])\n",
    "        if int(sv_len) > int(max_size) or int(sv_len) < int(min_size):\n",
    "            sample_SV.drop(index, inplace=True)\n",
    "    all_overlaps = []\n",
    "    min_max = []\n",
    "    for r1 in sample_SV.itertuples():\n",
    "        pos_overlap = \"\"\n",
    "        smallest = float(r1.START_POS)\n",
    "        largest = float(r1.STOP_POS)\n",
    "        for r2 in sample_SV.itertuples():  \n",
    "            if r1.CHROM == r2.CHROM:\n",
    "                if float(r2.START_POS) <= float(r1.START_POS) and float(r2.STOP_POS) < float(r1.STOP_POS) and float(r2.STOP_POS) > float(r1.START_POS):      \n",
    "                    caller = r2.CALLER\n",
    "                    start2 = float(r2.START_POS)\n",
    "                    stop2 = float(r2.STOP_POS)\n",
    "                    start1 = float(r1.START_POS)\n",
    "                    stop1 = float(r1.STOP_POS)\n",
    "                    sv_type = r2.SV_TYPE\n",
    "                    overlap = (stop2 - start1) / (stop1 - start1) * 100\n",
    "                    overlap_2 = (stop2 - start1) / (stop2 - start2) * 100\n",
    "                    if overlap >= float(percent_threshold) and overlap_2 >= float(percent_threshold):\n",
    "                        info = \"ID=\" + caller + ':' + \"START=\" + str(start2) + ':' + \"END=\" + str(stop2) + ':' + sv_type + ';' \n",
    "                        pos_overlap += info\n",
    "                        if start2 < smallest:\n",
    "                            smallest = start2\n",
    "                        elif stop2 > largest:\n",
    "                            largest = stop2\n",
    "                elif float(r2.START_POS) > float(r1.START_POS) and float(r2.STOP_POS) >= float(r1.STOP_POS) and float(r2.START_POS) < float(r1.STOP_POS):\n",
    "                    caller = r2.CALLER\n",
    "                    start2 = float(r2.START_POS)\n",
    "                    stop2 = float(r2.STOP_POS)\n",
    "                    start1 = float(r1.START_POS)\n",
    "                    stop1 = float(r1.STOP_POS)\n",
    "                    sv_type = r2.SV_TYPE\n",
    "                    overlap = (stop1 - start2) / (stop1 - start1) * 100\n",
    "                    overlap_2 = (stop1 - start2) / (stop2 - start2) * 100\n",
    "                    if overlap >= float(percent_threshold) and overlap_2 >= float(percent_threshold):\n",
    "                        info = \"ID=\" + caller + ':' + \"START=\" + str(start2) + ':' + \"END=\" + str(stop2) + ':' + sv_type + ';' \n",
    "                        pos_overlap += info \n",
    "                        if start2 < smallest:\n",
    "                            smallest = start2\n",
    "                        elif stop2 > largest:\n",
    "                            largest = stop2\n",
    "                elif float(r2.START_POS) <= float(r1.START_POS) and float(r2.STOP_POS) >= float(r1.STOP_POS):\n",
    "                    caller = r2.CALLER\n",
    "                    start2 = float(r2.START_POS)\n",
    "                    stop2 = float(r2.STOP_POS)\n",
    "                    start1 = float(r1.START_POS)\n",
    "                    stop1 = float(r1.STOP_POS)\n",
    "                    sv_type = r2.SV_TYPE\n",
    "                    overlap = (stop1 - start1) / (stop2 - start2) * 100\n",
    "                    if overlap >= float(percent_threshold):\n",
    "                        info = \"ID=\" + caller + ':' + \"START=\" + str(start2) + ':' + \"END=\" + str(stop2) + ':' + sv_type + ';' \n",
    "                        pos_overlap += info\n",
    "                        if start2 < smallest:\n",
    "                            smallest = start2\n",
    "                        elif stop2 > largest:\n",
    "                            largest = stop2\n",
    "\n",
    "                elif float(r2.START_POS) >= float(r1.START_POS) and float(r2.STOP_POS) <= float(r1.STOP_POS):\n",
    "                    caller = r2.CALLER\n",
    "                    start2 = float(r2.START_POS)\n",
    "                    stop2 = float(r2.STOP_POS)\n",
    "                    start1 = float(r1.START_POS)\n",
    "                    stop1 = float(r1.STOP_POS)\n",
    "                    sv_type = r2.SV_TYPE\n",
    "                    overlap = (stop2 - start2) / (stop1 - start1) * 100 \n",
    "\n",
    "                    if overlap >= float(percent_threshold):\n",
    "                        info = \"ID=\" + caller + ':' + \"START=\" + str(start2) + ':' + \"END=\" + str(stop2) + ':' + sv_type + ';' \n",
    "                        pos_overlap += info \n",
    "                        if start2 < smallest:\n",
    "                            smallest = start2\n",
    "                        elif stop2 > largest:\n",
    "                            largest = stop2\n",
    "                \n",
    "        all_overlaps.append(pos_overlap)\n",
    "        range_info = str(smallest) + '-' + str(largest)\n",
    "        min_max.append(range_info)\n",
    "    how_many = []\n",
    "    for item in all_overlaps:\n",
    "        each_type = item.split(';')\n",
    "        del each_type[-1]\n",
    "        all_types = []\n",
    "        for thing in each_type:\n",
    "            callerID = thing.split(':')[0]\n",
    "            all_types.append(callerID)\n",
    "        all_types = list(dict.fromkeys(all_types))\n",
    "        total = len(all_types)  \n",
    "        how_many.append(total)\n",
    "    sample_SV['OVERLAPS'] = all_overlaps\n",
    "    sample_SV['NUM_CALLERS'] = how_many\n",
    "    sample_SV['RANGE'] = min_max\n",
    "    dedup_SV = sample_SV.drop_duplicates('RANGE', keep='first')\n",
    "    dedup_SV_2 = dedup_SV.drop_duplicates('RANGE', keep='first')\n",
    "    high_num = dedup_SV_2.loc[dedup_SV_2['NUM_CALLERS'] >= caller_threshold]\n",
    "    ranges = high_num['RANGE'].to_list()\n",
    "    chroms = high_num['CHROM'].to_list()\n",
    "    callers = high_num['OVERLAPS'].to_list()\n",
    "    num_callers = high_num['NUM_CALLERS'].to_list()\n",
    "    all_starts = []\n",
    "    all_stops = []\n",
    "    lengths = []\n",
    "    for nums in ranges:\n",
    "        start = nums.split('-')[0]\n",
    "        stop = nums.split('-')[1]\n",
    "        length = float(stop) - float(start)\n",
    "        all_starts.append(start)\n",
    "        all_stops.append(stop)\n",
    "        lengths.append(length)    \n",
    "    everything = {'CHROM': chroms, 'START': all_starts, 'STOP': all_stops, 'SV_CALLED': callers, 'NUM_CALLERS': num_callers, 'SV_LEN': lengths}\n",
    "    new_frame = pd.DataFrame(everything)   \n",
    "    return new_frame\n",
    "  \n",
    "def most_frequent(List): \n",
    "    return max(set(List), key = List.count)\n",
    "\n",
    "def dedup_reformat(sample_table, over_t):\n",
    "\n",
    "    over_t = float(over_t)\n",
    "    for r1 in sample_table.itertuples():\n",
    "        for r2 in sample_table.itertuples():\n",
    "            if r1.Index != r2.Index:\n",
    "                if r1.CHROM == r2.CHROM:\n",
    "                    if float(r2.START) <= float(r1.START) and float(r2.STOP) < float(r1.STOP) and float(r2.STOP) > float(r1.START):\n",
    "                        overlap = (float(r2.STOP) - float(r1.START)) / (float(r1.STOP) - float(r1.START)) * 100\n",
    "                        overlap_2 = (float(r2.STOP) - float(r1.START)) / (float(r2.STOP) - float(r2.START)) * 100\n",
    "                        if overlap >= over_t and overlap_2 >= over_t:\n",
    "                            info1 = r1.SV_CALLED.split(';')\n",
    "                            info2 = r2.SV_CALLED.split(';')\n",
    "                            del info1[-1]\n",
    "                            del info2[-1]\n",
    "                            new_info = info1 + list(set(info2) - set(info1))\n",
    "                            info_str = ';'\n",
    "                            info_str = info_str.join(new_info) \n",
    "                            info_str += ';'\n",
    "                            new_length = float(r1.STOP) - float(r2.START)\n",
    "                            sample_table.at[r1.Index, 'SV_CALLED'] = info_str\n",
    "                            sample_table.at[r1.Index, 'START'] = float(r2.START)\n",
    "                            sample_table.at[r1.Index, 'STOP'] = float(r1.STOP)\n",
    "                            sample_table.at[r1.Index, 'CHROM'] = r1.CHROM\n",
    "                            sample_table.at[r1.Index, 'SV_LEN'] = new_length\n",
    "                            sample_table.at[r1.Index, 'NUM_CALLERS'] = r1.NUM_CALLERS\n",
    "                            sample_table.drop(r2.Index, inplace=True)\n",
    "                    elif float(r2.START) > float(r1.START) and float(r2.STOP) >= float(r1.STOP) and float(r2.START) < float(r1.STOP):\n",
    "                        overlap = (float(r1.STOP) - float(r2.START)) / (float(r1.STOP) - float(r1.START)) * 100\n",
    "                        overlap_2 = (float(r1.STOP) - float(r2.START)) / (float(r2.STOP) - float(r2.START)) * 100\n",
    "                        if overlap >= over_t and overlap_2 >= over_t:\n",
    "                            info1 = r1.SV_CALLED.split(';')\n",
    "                            info2 = r2.SV_CALLED.split(';')\n",
    "                            del info1[-1]\n",
    "                            del info2[-1]\n",
    "                            new_info = info1 + list(set(info2) - set(info1))\n",
    "                            info_str = ';'\n",
    "                            info_str = info_str.join(new_info) \n",
    "                            info_str += ';'\n",
    "                            new_length = float(r2.STOP) - float(r1.START)\n",
    "                            sample_table.at[r1.Index, 'SV_CALLED'] = info_str\n",
    "                            sample_table.at[r1.Index, 'CHROM'] = r1.CHROM\n",
    "                            sample_table.at[r1.Index, 'SV_LEN'] = new_length\n",
    "                            sample_table.at[r1.Index, 'NUM_CALLERS'] = r1.NUM_CALLERS\n",
    "                            sample_table.at[r1.Index, 'STOP'] = float(r2.STOP)\n",
    "                            sample_table.at[r1.Index, 'START'] = float(r1.START)\n",
    "                            sample_table.drop(r2.Index, inplace=True)\n",
    "                    elif float(r2.START) <= float(r1.START) and float(r2.STOP) >= float(r1.STOP):\n",
    "                        overlap = (float(r1.STOP) - float(r1.START)) / (float(r2.STOP) - float(r2.START)) * 100\n",
    "                        if overlap >= over_t:\n",
    "                            info1 = r1.SV_CALLED.split(';')\n",
    "                            info2 = r2.SV_CALLED.split(';')\n",
    "                            del info1[-1]\n",
    "                            del info2[-1]\n",
    "                            new_info = info1 + list(set(info2) - set(info1))\n",
    "                            info_str = ';'\n",
    "                            info_str = info_str.join(new_info)\n",
    "                            info_str += ';'\n",
    "                            new_length = float(r2.STOP) - float(r2.START)\n",
    "                            sample_table.at[r1.Index, 'SV_CALLED'] = info_str\n",
    "                            sample_table.at[r1.Index, 'START'] = float(r2.START)\n",
    "                            sample_table.at[r1.Index, 'STOP'] = float(r2.STOP)\n",
    "                            sample_table.at[r1.Index, 'CHROM'] = r1.CHROM\n",
    "                            sample_table.at[r1.Index, 'SV_LEN'] = new_length\n",
    "                            sample_table.at[r1.Index, 'NUM_CALLERS'] = r1.NUM_CALLERS\n",
    "                            sample_table.drop(r2.Index, inplace=True)\n",
    "                    elif float(r2.START) >= float(r1.START) and float(r2.STOP) <= float(r1.STOP):\n",
    "                        overlap = (float(r2.STOP) - float(r2.START)) / (float(r1.STOP) - float(r1.START)) * 100\n",
    "                        if overlap >= over_t:\n",
    "                            info1 = r1.SV_CALLED.split(';')\n",
    "                            info2 = r2.SV_CALLED.split(';')\n",
    "                            del info1[-1]\n",
    "                            del info2[-1]\n",
    "                            new_info = info1 + list(set(info2) - set(info1))\n",
    "                            info_str = ';'\n",
    "                            info_str = info_str.join(new_info) \n",
    "                            info_str += ';'\n",
    "                            sample_table.at[r1.Index, 'SV_CALLED'] = info_str\n",
    "                            sample_table.at[r1.Index, 'START'] = float(r1.START)\n",
    "                            sample_table.at[r1.Index, 'STOP'] = float(r1.STOP)\n",
    "                            sample_table.at[r1.Index, 'CHROM'] = r1.CHROM\n",
    "                            sample_table.at[r1.Index, 'SV_LEN'] = r1.SV_LEN\n",
    "                            sample_table.at[r1.Index, 'NUM_CALLERS'] = r1.NUM_CALLERS\n",
    "                            sample_table.drop(r2.Index, inplace=True)\n",
    "\n",
    "    breakdancer = []\n",
    "    pindel = []\n",
    "    lumpy = []\n",
    "    manta = []\n",
    "    gridss = []\n",
    "    delly = []\n",
    "\n",
    "    for row1 in sample_table.itertuples():\n",
    "        br_ss = []\n",
    "        br_t = []\n",
    "        pi_ss = []\n",
    "        pi_t = []\n",
    "        lu_ss = []\n",
    "        lu_t = []\n",
    "        ma_ss = []\n",
    "        ma_t = []\n",
    "        gr_ss = []\n",
    "        gr_t = []\n",
    "        de_ss = []\n",
    "        de_t = []\n",
    "        ind = row1.SV_CALLED.split(';')\n",
    "        del ind[-1]\n",
    "        for thing in ind:\n",
    "            each = thing.split(':')\n",
    "            ID = each[0].split('=')[1]\n",
    "            START = each[1].split('=')[1]\n",
    "            STOP = each[2].split('=')[1]\n",
    "            TYPE = each[3]\n",
    "            if ID == 'breakdancer':\n",
    "                br_ss.append(float(START))\n",
    "                br_ss.append(float(STOP))\n",
    "                br_t.append(TYPE)\n",
    "            elif ID == 'pindel':\n",
    "                pi_ss.append(float(START))\n",
    "                pi_ss.append(float(STOP))\n",
    "                pi_t.append(TYPE)\n",
    "            elif ID == 'lumpy':\n",
    "                lu_ss.append(float(START))\n",
    "                lu_ss.append(float(STOP))\n",
    "                lu_t.append(TYPE)\n",
    "            elif ID == 'delly':\n",
    "                de_ss.append(float(START))\n",
    "                de_ss.append(float(STOP))\n",
    "                de_t.append(TYPE) \n",
    "            elif ID == 'manta':\n",
    "                ma_ss.append(float(START))\n",
    "                ma_ss.append(float(STOP))\n",
    "                ma_t.append(TYPE)\n",
    "            elif ID == 'gridss':\n",
    "                gr_ss.append(float(START))\n",
    "                gr_ss.append(float(STOP))\n",
    "                gr_t.append(TYPE)      \n",
    "        if len(br_ss) != 0:\n",
    "            s = float(min(br_ss))\n",
    "            e = float(max(br_ss))\n",
    "            percent_o = (e - s) / (float(row1.STOP) - float(row1.START)) * 100\n",
    "            ninfo = str(s) + '-' + str(e) + ':' + most_frequent(br_t) + ':' + str(round(percent_o, 2)) + '%'\n",
    "            breakdancer.append(ninfo)\n",
    "        if len(br_t) == 0:\n",
    "            x = 'X'\n",
    "            breakdancer.append(x)\n",
    "        if len(pi_ss) != 0:\n",
    "            s = float(min(pi_ss))\n",
    "            e = float(max(pi_ss))\n",
    "            percent_o = (e - s) / (float(row1.STOP) - float(row1.START)) * 100\n",
    "            ninfo = str(s) + '-' + str(e) + ':' + most_frequent(pi_t) + ':' + str(round(percent_o, 2)) + '%'\n",
    "            pindel.append(ninfo)\n",
    "        if len(pi_t) == 0:\n",
    "            x = 'X'\n",
    "            pindel.append(x)\n",
    "        if len(lu_ss) != 0:\n",
    "            s = float(min(lu_ss))\n",
    "            e = float(max(lu_ss))\n",
    "            percent_o = (e - s) / (float(row1.STOP) - float(row1.START)) * 100\n",
    "            ninfo = str(s) + '-' + str(e) + ':' + most_frequent(lu_t) + ':' + str(round(percent_o, 2)) + '%'\n",
    "            lumpy.append(ninfo)\n",
    "        if len(lu_t) == 0:\n",
    "            x = 'X'\n",
    "            lumpy.append(x)\n",
    "        if len(de_ss) != 0:\n",
    "            s = float(min(de_ss))\n",
    "            e = float(max(de_ss))\n",
    "            percent_o = (e - s) / (float(row1.STOP) - float(row1.START)) * 100\n",
    "            ninfo = str(s) + '-' + str(e) + ':' + most_frequent(de_t) + ':' + str(round(percent_o, 2)) + '%'\n",
    "            delly.append(ninfo)\n",
    "        if len(de_t) == 0:\n",
    "            x = 'X'\n",
    "            delly.append(x)\n",
    "        if len(ma_ss) != 0:\n",
    "            s = float(min(ma_ss))\n",
    "            e = float(max(ma_ss))\n",
    "            percent_o = (e - s) / (float(row1.STOP) - float(row1.START)) * 100\n",
    "            ninfo = str(s) + '-' + str(e) + ':' + most_frequent(ma_t) + ':' + str(round(percent_o, 2)) + '%'\n",
    "            manta.append(ninfo)\n",
    "        if len(ma_t) == 0:\n",
    "            x = 'X'\n",
    "            manta.append(x)\n",
    "        if len(gr_ss) != 0:\n",
    "            s = float(min(gr_ss))\n",
    "            e = float(max(gr_ss))\n",
    "            percent_o = (e - s) / (float(row1.STOP) - float(row1.START)) * 100\n",
    "            ninfo = str(s) + '-' + str(e) + ':' + most_frequent(gr_t) + ':' + str(round(percent_o, 2)) + '%'\n",
    "            gridss.append(ninfo)\n",
    "        if len(gr_t) == 0:\n",
    "            x = 'X'\n",
    "            gridss.append(x)\n",
    "    starts = sample_table['START'].to_list()\n",
    "    stops = sample_table['STOP'].to_list()\n",
    "    chroms = sample_table['CHROM'].to_list()\n",
    "    num_callers = sample_table['NUM_CALLERS'].to_list()\n",
    "    all_lengths = sample_table['SV_LEN'].to_list()\n",
    "    all_reformat = {'CHROM': chroms, 'START': starts, 'STOP': stops,'NUM_CALLERS': num_callers, 'SV_LEN': all_lengths, 'BREAKDANCER': breakdancer, 'PINDEL': pindel, 'LUMPY': lumpy, 'MANTA': manta, 'DELLY': delly, 'GRIDSS': gridss}\n",
    "    reformat_frame = pd.DataFrame(all_reformat)\n",
    "    return reformat_frame\n",
    "\n",
    "def overlap_samples(all_samples, over_t):\n",
    "    all_samples['SAMPLE_COUNTS'] = ''\n",
    "    over_t = int(over_t)\n",
    "    for r1 in all_samples.itertuples():\n",
    "        all_callers = []\n",
    "        for r2 in all_samples.itertuples():\n",
    "            if r1.Index != r2.Index:\n",
    "                if r1.SAMPLE != r2.SAMPLE:\n",
    "                    if r1.CHROM == r2.CHROM:\n",
    "                        if float(r2.START) <= float(r1.START) and float(r2.STOP) < float(r1.STOP) and float(r2.STOP) > float(r1.START):\n",
    "                            overlap = (float(r2.STOP) - float(r1.START)) / (float(r1.STOP) - float(r1.START)) * 100\n",
    "                            overlap_2 = (float(r2.STOP) - float(r1.START)) / (float(r2.STOP) - float(r2.START)) * 100\n",
    "                            if overlap >= over_t and overlap_2 >= over_t:\n",
    "                                if r1.SAMPLE_COUNTS == '' and r2.SAMPLE_COUNTS == '':\n",
    "                                    new_sample_info = str(r1.SAMPLE) + ':' + str(r2.SAMPLE)\n",
    "                                elif r1.SAMPLE_COUNTS == '' and r2.SAMPLE_COUNTS != '':\n",
    "                                    new_sample_info = str(r1.SAMPLE) + ':' + str(r2.SAMPLE_COUNTS)\n",
    "                                elif r1.SAMPLE_COUNTS != '' and r2.SAMPLE_COUNTS == '':\n",
    "                                    new_sample_info = str(r1.SAMPLE_COUNTS) + ':' + str(r2.SAMPLE)\n",
    "                                else:\n",
    "                                    info1 = r1.SAMPLE_COUNTS.split(':')\n",
    "                                    info2 = r2.SAMPLE_COUNTS.split(':')\n",
    "                                    new_info = info1 + list(set(info2) - set(info1))\n",
    "                                    new_sample_info = ':'\n",
    "                                    new_sample_info = new_sample_info.join(new_info)\n",
    "\n",
    "                                new_b = r1.BREAKDANCER + ';' + r2.BREAKDANCER\n",
    "                                new_p = r1.PINDEL + ';' + r2.PINDEL\n",
    "                                new_l = r1.LUMPY + ';' + r2.LUMPY\n",
    "                                new_m = r1.MANTA + ';' + r2.MANTA\n",
    "                                new_d = r1.DELLY + ';' + r2.DELLY\n",
    "                                new_g = r1.GRIDSS + ';' + r2.GRIDSS\n",
    "                                new_length = float(r1.STOP) - float(r2.START)\n",
    "                                all_samples.at[r1.Index, 'START'] = r2.START\n",
    "                                all_samples.at[r1.Index, 'SAMPLE'] = r1.SAMPLE\n",
    "                                all_samples.at[r1.Index, 'SAMPLE_COUNTS'] = new_sample_info\n",
    "                                all_samples.at[r1.Index, 'STOP'] = r1.STOP\n",
    "                                all_samples.at[r1.Index, 'CHROM'] = r1.CHROM\n",
    "                                all_samples.at[r1.Index, 'SV_LEN'] = new_length\n",
    "                                all_samples.at[r1.Index, 'NUM_CALLERS'] = r1.NUM_CALLERS\n",
    "                                all_samples.at[r1.Index, 'BREAKDANCER'] = new_b\n",
    "                                all_samples.at[r1.Index, 'PINDEL'] = new_p\n",
    "                                all_samples.at[r1.Index, 'LUMPY'] = new_l\n",
    "                                all_samples.at[r1.Index, 'MANTA'] = new_m\n",
    "                                all_samples.at[r1.Index, 'DELLY'] = new_d\n",
    "                                all_samples.at[r1.Index, 'GRIDSS'] = new_d\n",
    "                                all_samples.drop(r2.Index, inplace=True)\n",
    "\n",
    "                        elif float(r2.START) > float(r1.START) and float(r2.STOP) >= float(r1.STOP) and float(r2.START) < float(r1.STOP):\n",
    "                            overlap = (float(r1.STOP) - float(r2.START)) / (float(r1.STOP) - float(r1.START)) * 100\n",
    "                            overlap_2 = (float(r1.STOP) - float(r2.START)) / (float(r2.STOP) - float(r2.START)) * 100\n",
    "                            if overlap >= over_t and overlap_2 >= over_t:\n",
    "                                if r1.SAMPLE_COUNTS == '' and r2.SAMPLE_COUNTS == '':\n",
    "                                    new_sample_info = str(r1.SAMPLE) + ':' + str(r2.SAMPLE)\n",
    "                                elif r1.SAMPLE_COUNTS == '' and r2.SAMPLE_COUNTS != '':\n",
    "                                    new_sample_info = str(r1.SAMPLE) + ':' + str(r2.SAMPLE_COUNTS)\n",
    "                                elif r1.SAMPLE_COUNTS != '' and r2.SAMPLE_COUNTS == '':\n",
    "                                    new_sample_info = str(r1.SAMPLE_COUNTS) + ':' + str(r2.SAMPLE)\n",
    "                                else:\n",
    "                                    info1 = r1.SAMPLE_COUNTS.split(':')\n",
    "                                    info2 = r2.SAMPLE_COUNTS.split(':')\n",
    "                                    new_info = info1 + list(set(info2) - set(info1))\n",
    "                                    new_sample_info = ':'\n",
    "                                    new_sample_info = new_sample_info.join(new_info)\n",
    "                                new_length = float(r2.STOP) - float(r1.START)\n",
    "                                new_b = r1.BREAKDANCER + ';' + r2.BREAKDANCER\n",
    "                                new_p = r1.PINDEL + ';' + r2.PINDEL\n",
    "                                new_l = r1.LUMPY + ';' + r2.LUMPY\n",
    "                                new_m = r1.MANTA + ';' + r2.MANTA\n",
    "                                new_d = r1.DELLY + ';' + r2.DELLY\n",
    "                                new_g = r1.GRIDSS + ';' + r2.GRIDSS\n",
    "                                all_samples.at[r1.Index, 'SAMPLE_COUNTS'] = new_sample_info\n",
    "                                all_samples.at[r1.Index, 'CHROM'] = r1.CHROM\n",
    "                                all_samples.at[r1.Index, 'SV_LEN'] = new_length\n",
    "                                all_samples.at[r1.Index, 'NUM_CALLERS'] = r1.NUM_CALLERS\n",
    "                                all_samples.at[r1.Index, 'STOP'] = r2.STOP\n",
    "                                all_samples.at[r1.Index, 'START'] = r1.START\n",
    "                                all_samples.at[r1.Index, 'SAMPLE'] = r1.SAMPLE\n",
    "                                all_samples.at[r1.Index, 'BREAKDANCER'] = new_b\n",
    "                                all_samples.at[r1.Index, 'PINDEL'] = new_p\n",
    "                                all_samples.at[r1.Index, 'LUMPY'] = new_l\n",
    "                                all_samples.at[r1.Index, 'MANTA'] = new_m\n",
    "                                all_samples.at[r1.Index, 'DELLY'] = new_d\n",
    "                                all_samples.at[r1.Index, 'GRIDSS'] = new_d\n",
    "                                all_samples.drop(r2.Index, inplace=True)\n",
    "\n",
    "                        elif float(r2.START) <= float(r1.START) and float(r2.STOP) >= float(r1.STOP):\n",
    "\n",
    "                            overlap = (float(r1.STOP) - float(r1.START)) / (float(r2.STOP) - float(r2.START)) * 100\n",
    "                            if overlap >= over_t:\n",
    "                                if r1.SAMPLE_COUNTS == '' and r2.SAMPLE_COUNTS == '':\n",
    "                                    new_sample_info = str(r1.SAMPLE) + ':' +  str(r2.SAMPLE)\n",
    "                                elif r1.SAMPLE_COUNTS == '' and r2.SAMPLE_COUNTS != '':\n",
    "                                    new_sample_info = str(r1.SAMPLE) + ':' + str(r2.SAMPLE_COUNTS)\n",
    "                                elif r1.SAMPLE_COUNTS != '' and r2.SAMPLE_COUNTS == '':\n",
    "                                    new_sample_info = str(r1.SAMPLE_COUNTS) + ':' +  str(r2.SAMPLE)\n",
    "                                else:\n",
    "                                    info1 = r1.SAMPLE_COUNTS.split(':')\n",
    "                                    info2 = r2.SAMPLE_COUNTS.split(':')\n",
    "                                    new_info = info1 + list(set(info2) - set(info1))\n",
    "                                    new_sample_info = ':'\n",
    "                                    new_sample_info = new_sample_info.join(new_info)\n",
    "                                new_length = float(r2.STOP) - float(r2.START)\n",
    "                                new_b = r1.BREAKDANCER + ';' + r2.BREAKDANCER\n",
    "                                new_p = r1.PINDEL + ';' + r2.PINDEL\n",
    "                                new_l = r1.LUMPY + ';' + r2.LUMPY\n",
    "                                new_m = r1.MANTA + ';' + r2.MANTA\n",
    "                                new_d = r1.DELLY + ';' + r2.DELLY\n",
    "                                new_g = r1.GRIDSS + ';' + r2.GRIDSS\n",
    "                                all_samples.at[r1.Index, 'SAMPLE_COUNTS'] = new_sample_info\n",
    "                                all_samples.at[r1.Index, 'START'] = r2.START\n",
    "                                all_samples.at[r1.Index, 'STOP'] = r2.STOP\n",
    "                                all_samples.at[r1.Index, 'CHROM'] = r1.CHROM\n",
    "                                all_samples.at[r1.Index, 'SV_LEN'] = new_length\n",
    "                                all_samples.at[r1.Index, 'NUM_CALLERS'] = r1.NUM_CALLERS\n",
    "                                all_samples.at[r1.Index, 'SAMPLE'] = r1.SAMPLE\n",
    "                                all_samples.at[r1.Index, 'BREAKDANCER'] = new_b\n",
    "                                all_samples.at[r1.Index, 'PINDEL'] = new_p\n",
    "                                all_samples.at[r1.Index, 'LUMPY'] = new_l\n",
    "                                all_samples.at[r1.Index, 'MANTA'] = new_m\n",
    "                                all_samples.at[r1.Index, 'DELLY'] = new_d\n",
    "                                all_samples.at[r1.Index, 'GRIDSS'] = new_d\n",
    "                                all_samples.drop(r2.Index, inplace=True)\n",
    "\n",
    "\n",
    "                        elif float(r2.START) >= float(r1.START) and float(r2.STOP) <= float(r1.STOP):\n",
    "\n",
    "                            overlap = (float(r2.STOP) - float(r2.START)) / (float(r1.STOP) - float(r1.START)) * 100\n",
    "                            if overlap >= over_t:\n",
    "                                if r1.SAMPLE_COUNTS == '' and r2.SAMPLE_COUNTS == '':\n",
    "                                    new_sample_info = str(r1.SAMPLE) + ':' +  str(r2.SAMPLE)\n",
    "                                elif r1.SAMPLE_COUNTS == '' and r2.SAMPLE_COUNTS != '':\n",
    "                                    new_sample_info = str(r1.SAMPLE) + ':' + str(r2.SAMPLE_COUNTS)\n",
    "                                elif r1.SAMPLE_COUNTS != '' and r2.SAMPLE_COUNTS == '':\n",
    "                                    new_sample_info = str(r1.SAMPLE_COUNTS) + ':' + str(r2.SAMPLE)\n",
    "                                else:\n",
    "                                    info1 = r1.SAMPLE_COUNTS.split(':')\n",
    "                                    info2 = r2.SAMPLE_COUNTS.split(':')\n",
    "                                    new_info = info1 + list(set(info2) - set(info1))\n",
    "                                    new_sample_info = ':'\n",
    "                                    new_sample_info = new_sample_info.join(new_info)\n",
    "                                new_b = r1.BREAKDANCER + ';' + r2.BREAKDANCER\n",
    "                                new_p = r1.PINDEL + ';' + r2.PINDEL\n",
    "                                new_l = r1.LUMPY + ';' + r2.LUMPY\n",
    "                                new_m = r1.MANTA + ';' + r2.MANTA\n",
    "                                new_d = r1.DELLY + ';' + r2.DELLY\n",
    "                                new_g = r1.GRIDSS + ';' + r2.GRIDSS\n",
    "                                all_samples.at[r1.Index, 'SAMPLE_COUNTS'] = new_sample_info\n",
    "                                all_samples.at[r1.Index, 'START'] = r1.START\n",
    "                                all_samples.at[r1.Index, 'STOP'] = r1.STOP\n",
    "                                all_samples.at[r1.Index, 'CHROM'] = r1.CHROM\n",
    "                                all_samples.at[r1.Index, 'SV_LEN'] = r1.SV_LEN\n",
    "                                all_samples.at[r1.Index, 'NUM_CALLERS'] = r1.NUM_CALLERS\n",
    "                                all_samples.at[r1.Index, 'SAMPLE'] = r1.SAMPLE\n",
    "                                all_samples.at[r1.Index, 'BREAKDANCER'] = new_b\n",
    "                                all_samples.at[r1.Index, 'PINDEL'] = new_p\n",
    "                                all_samples.at[r1.Index, 'LUMPY'] = new_l\n",
    "                                all_samples.at[r1.Index, 'MANTA'] = new_m\n",
    "                                all_samples.at[r1.Index, 'DELLY'] = new_d\n",
    "                                all_samples.at[r1.Index, 'GRIDSS'] = new_d\n",
    "                                all_samples.drop(r2.Index, inplace=True)\n",
    "\n",
    "    all_samples\n",
    "    how_many = []\n",
    "    dedup_samples = []\n",
    "    for row in all_samples.itertuples():\n",
    "        if row.SAMPLE_COUNTS == '':\n",
    "            how_many.append(1)\n",
    "            dedup_samples.append(str(row.SAMPLE))\n",
    "        else:\n",
    "            called_sams = row.SAMPLE_COUNTS.split(':')\n",
    "            called_sams = list(dict.fromkeys(called_sams))\n",
    "            total = len(called_sams)\n",
    "            how_many.append(total)\n",
    "            the_samples = ':'\n",
    "            the_samples = the_samples.join(called_sams)\n",
    "            dedup_samples.append(the_samples)\n",
    "\n",
    "\n",
    "    all_samples['SAMPLE_COUNTS'] = dedup_samples\n",
    "    all_samples['NUM_SAMPLES'] = how_many\n",
    "    all_samples.sort_values(by='CHROM', inplace=True)\n",
    "    return all_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-survey",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "cd /path/to/data\n",
    "mkdir -p allSV \n",
    "mkdir -p mergeSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-surgeon",
   "metadata": {
    "kernel": "python3"
   },
   "outputs": [],
   "source": [
    "#reformats output from files to make compatible \n",
    "\n",
    "pathToDir = \"path/to/data/\"\n",
    "\n",
    "for num in samples:\n",
    "    fName = pathToDir + \"breakdancer/\" + 'AS' + str(num) + '_breakdancer.txt'\n",
    "    outfile = pathToDir + 'AS' + str(num) + \".reformat.vcf\"\n",
    "    reformat = reformat_breakdancer(fName)\n",
    "    reformat.to_csv(outfile, sep='\\t',index=False)\n",
    "    \n",
    "    break_file = pathToDir + \"breakdancer/\" + \"AS\" + num + '.reformat.vcf'\n",
    "    breakdancer = read_vcf(break_file)\n",
    "    reformat_b = reformat_ss(breakdancer, 'breakdancer')\n",
    "    output_fileb = 'pathToDirbreakdancer/' + \"AS\" + num + '_breakdancer.ss.vcf'\n",
    "    reformat_b.to_csv(output_fileb, sep='\\t', index=False)\n",
    "    \n",
    "    manta_file = pathToDir + 'manta/' + \"AS\" + num + '_manta.vcf'\n",
    "    manta = read_vcf(manta_file)\n",
    "    reformat_m = reformat_ss(manta, 'manta')\n",
    "    output_filem = pathToDir + 'manta/' + \"AS\" + num + '_manta.ss.vcf'\n",
    "    reformat_m.to_csv(output_filem, sep='\\t', index=False)\n",
    "    \n",
    "    delly_file = pathToDir + 'delly/' + \"AS\" + num + '.vcf'\n",
    "    delly = read_vcf(delly_file)\n",
    "    reformat_d = reformat_ss(delly, 'delly')\n",
    "    output_filed = pathToDir 'delly/' + \"AS\" + num + '_delly.ss.vcf'\n",
    "    reformat_d.to_csv(output_filed, sep='\\t', index=False)\n",
    "    \n",
    "    pindel_file = pathToDir + 'pindel/' + \"AS\" + num + '_pindel.vcf'\n",
    "    pindel = read_vcf(pindel_file)\n",
    "    reformat_p = reformat_ss(pindel, 'pindel')\n",
    "    output_filep = pathToDir + 'pindel/' + \"AS\" + num + '_pindel.ss.vcf'\n",
    "    reformat_p.to_csv(output_filep, sep='\\t', index=False)\n",
    "    \n",
    "    gridss_file = pathToDir + 'gridss/' + \"AS\" + num + '_gridss.ann.bed'\n",
    "    gridss = pd.read_csv(gridss_file, sep='\\t', names=['CHROM', 'START_POS', 'STOP_POS', 'SV_TYPE', 'QUAL', 'CALLER'])\n",
    "    gridss['CALLER'] = 'gridss'\n",
    "    output_fileg = pathToDir + 'gridss/' + \"AS\" + num + '_gridss.ss.vcf'\n",
    "    gridss.to_csv(output_fileg, sep='\\t', index=False)\n",
    "    \n",
    "    lumpy_file = pathToDir + 'lumpy/' + \"AS\" + num + '_lumpy.ann.bed'\n",
    "    lumpy = pd.read_csv(lumpy_file, sep='\\t', names=['CHROM', 'START_POS', 'STOP_POS', 'SV_TYPE', 'QUAL', 'CALLER'])\n",
    "    lumpy['CALLER'] = 'lumpy'\n",
    "    output_filel = pathToDir + 'lumpy/' + \"AS\" + num + '_lumpy.ss.vcf'\n",
    "    lumpy.to_csv(output_filel, sep='\\t', index=False)\n",
    " \n",
    "    combined_table = combine_SV(output_fileb,output_filep,output_filed,output_filem,output_filel,output_fileg)\n",
    "    out_file = 'pathToDirallSV/' + 'AS' + num + '_allSV.txt'\n",
    "    combined_table.to_csv(out_file, sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-employee",
   "metadata": {
    "kernel": "python3"
   },
   "outputs": [],
   "source": [
    "#takes all concat SV files frome each sample and overlap the calls to make a merged file SV\n",
    "\n",
    "path = \"/path/to/data/allSV/\"\n",
    "\n",
    "#INSERT LIST OF SAMPLES\n",
    "##sams = [\"149\", \"524\", \"525\", \"542\", \"553\",\"581\", \"591\", \"610\", \"625\", \"626\", \"647\", \"697\", \"717\"]\n",
    "for num in sams:\n",
    "    file = path + 'AS' + num  + '_allSV.txt'\n",
    "    outfile = path + 'AS' + num  + '_mergeSV_all.txt'\n",
    "    sample_table = make_merged(file,10000000000,1,75,3)\n",
    "    new_sample_table = dedup_reformat(sample_table, 70)\n",
    "    new_sample_table.to_csv(outfile, sep='\\t')\n",
    "\n",
    "\n",
    "#reformats first column of each sample\n",
    "for num in sams:\n",
    "    samDf = path + 'AS' + num + '_mergeSV_all.txt'\n",
    "    samFrame = pd.read_csv(samDf, sep='\\t')\n",
    "    samFrame.rename(columns={'Unnamed: 0': 'SAMPLE'}, inplace=True)\n",
    "    samID = 'AS' + num\n",
    "    samFrame['SAMPLE'] = samID\n",
    "    outfile = path + 'AS' + num  + '_mergeSV_all.txt'\n",
    "    samFrame.to_csv(outfile, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-orleans",
   "metadata": {
    "kernel": "python3"
   },
   "outputs": [],
   "source": [
    "#reads in each samples merged calls and and then overlaps between samples \n",
    "###FIGURE OUT BETTER WAY OF DOING THIS\n",
    "bm149 = pd.read_csv('/path/to/data/allSV/bm149_mergeSV_all.txt', sep='\\t')\n",
    "bm525 = pd.read_csv('/path/to/data/allSV/bm525_mergeSV_all.txt', sep='\\t')\n",
    "bm542 = pd.read_csv('/path/to/data/allSV/bm542_mergeSV_all.txt', sep='\\t')\n",
    "bm553 = pd.read_csv('/path/to/data/allSV/bm553_mergeSV_all.txt', sep='\\t')\n",
    "bm581 = pd.read_csv('/path/to/data/allSV/bm581_mergeSV_all.txt', sep='\\t')\n",
    "bm591 = pd.read_csv('/path/to/data/allSV/bm591_mergeSV_all.txt', sep='\\t')\n",
    "bm610 = pd.read_csv('/path/to/data/allSV/bm610_mergeSV_all.txt', sep='\\t')\n",
    "bm625 = pd.read_csv('/path/to/data/allSV/bm625_mergeSV_all.txt', sep='\\t')\n",
    "bm626 = pd.read_csv('/path/to/data/allSV/bm626_mergeSV_all.txt', sep='\\t')\n",
    "bm647 = pd.read_csv('/path/to/data/allSV/bm647_mergeSV_all.txt', sep='\\t')\n",
    "bm697 = pd.read_csv('/path/to/data/allSV/bm697_mergeSV_all.txt', sep='\\t')\n",
    "bm717 = pd.read_csv('/path/to/data/allSV/bm717_mergeSV_all.txt', sep='\\t')\n",
    "\n",
    "###FIX\n",
    "all_samples = pd.concat([bm149,bm524, bm525, bm542, bm553, bm581, bm591, bm610, bm625, bm626, bm647, bm697, bm717], axis=0, ignore_index=True)\n",
    "\n",
    "#overlaps at 75% to consider the \"same\"\n",
    "all_samples = overlap_samples(all_samples, 75)\n",
    "all_samples.sort_values(by=['CHROM','START'], inplace=True)\n",
    "\n",
    "#Removes any SV that all samples share since indicates ref \n",
    "remove = all_samples.loc[all_samples['NUM_SAMPLES'] < 13]\n",
    "remove.to_csv(\"/path/to/data/mergeSV/noShareSV.txt\", sep=\"\\t\", index=False)\n",
    "bed_format = remove.filter(['CHROM','START','STOP'], axis=1)\n",
    "bed_format.to_csv('/path/to/data/mergeSV/noShareSV.bed', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-commonwealth",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "cd /path/to/data/mergeSV\n",
    "mkdir -p finalSV\n",
    "bedtools igv -slop 10 -path /path/to/data/mergeSV -i /path/to/data/mergeSV/noShareSV.bed > /path/to/data/finalSV/otherScript.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-release",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "cd /path/to/data/mergeSV/finalSV\n",
    "for photo in *.png ; do convert $photo -rotate -90 $photo ; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-johnston",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-cedar",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "automated-thumbnail",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "SNP/INDEL calling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-desperate",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "#John Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-remains",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "Phylogeny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-squad",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-junction",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "calysto_bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "python3",
     "python3",
     "python",
     "",
     ""
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0
   },
   "version": "0.20.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
